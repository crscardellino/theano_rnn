{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Neural Network to Train a Language Model on Borges' Work\n",
    "\n",
    "In this notebook I will write the steps in order to train a character level language model (similar to [Andrej Karpathy's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)), based on the work by argentinian author Jorge Luis Borges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue: Getting Borges' Work\n",
    "\n",
    "Before starting with the recurrent neural network, we should go ahead and get the work of Borges. Naturally, the more data you have, the more precise the network will be. For this, we will start with getting some of the work of J.L.B. in plain text format.\n",
    "\n",
    "Once we have the whole corpus, we need a way to encode each of the characters. We'll use the one-hot encoding, where each character is encoded as a vector with `1` in it's corresponding position. To do so we need to get all the different characters from the corpus.\n",
    "\n",
    "Finally we just need an method to encode all the characters into a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus = {}\n",
    "        self.length = 0\n",
    "\n",
    "        for fname in os.listdir(corpus_path):\n",
    "            fpath = os.path.join(corpus_path, fname)\n",
    "\n",
    "            with open(fpath, \"r\") as f:\n",
    "                self.corpus[fname.replace(\".txt\", \"\")] = f.read().decode(\"utf-8\")\n",
    "\n",
    "        characters = set()\n",
    "\n",
    "        for work_name, work in self.corpus.iteritems():\n",
    "            for c in work:\n",
    "                characters.add(c)\n",
    "                self.length += 1\n",
    "\n",
    "        self.characters = sorted(characters)\n",
    "        \n",
    "    def character_encoder(self, char):\n",
    "        vector = np.zeros((len(self.characters),), dtype='int64')\n",
    "        vector[self.characters.index(char)] = 1\n",
    "\n",
    "        return vector\n",
    "\n",
    "    def __iter__(self):\n",
    "        for work_name, work in self.corpus.iteritems():\n",
    "            for char in work:\n",
    "                yield self.character_encoder(char)\n",
    "            yield self.character_encoder(u\"\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "corpus = Corpus(\"corpus/borges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## First Approach: Simple RNN\n",
    "\n",
    "For our first approach, we will write a class for a Simple Recurrent Neural Network. This is RNN with a non-gated unit.\n",
    "\n",
    "Here, we begin by setting up some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NT = len(corpus)  # Number of examples (timesteps)\n",
    "n_in = len(corpus.characters)  # Size of the input data (one-hot vector of a character)\n",
    "n_out = len(corpus.characters)  # Size of the output data (one-hot vector of a character)\n",
    "n_h = 50  # Size of the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue to set the theano graph for a Simple Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stateless variables to handle the input\n",
    "X = T.matrix('X')\n",
    "y = T.lvector('y')\n",
    "\n",
    "W_hx = theano.shared(\n",
    "    value=np.random.uniform(\n",
    "        low=-1.0,\n",
    "        high=1.0,\n",
    "        size=(n_in, n_h)\n",
    "    ).astype(theano.config.floatX),\n",
    "    name='W_hx',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "b_h = theano.shared(\n",
    "    value=np.zeros(n_h, dtype=theano.config.floatX),\n",
    "    name='b_h',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "W_hh = theano.shared(\n",
    "    value=np.random.uniform(\n",
    "        low=-1.0,\n",
    "        high=1.0,\n",
    "        size=(n_h, n_h)\n",
    "    ).astype(theano.config.floatX),\n",
    "    name='W_hh',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "W_S = theano.shared(\n",
    "    value=np.random.uniform(\n",
    "        low=-1.0,\n",
    "        high=1.0,\n",
    "        size=(n_h, n_out)\n",
    "    ).astype(theano.config.floatX),\n",
    "    name='W_S',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "b_S = theano.shared(\n",
    "    value=np.zeros(n_out, dtype=theano.config.floatX),\n",
    "    name='b_S',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "h0 = theano.shared(\n",
    "    value=np.zeros(n_h, dtype=theano.config.floatX),\n",
    "    name='h0',\n",
    "    borrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the flow for forward propagation. We need to save all the hidden states, as we need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propagation_step(x_t, h_t_prev, W_hx, W_hh, b_h, W_S, b_S):\n",
    "    h_t = T.tanh(T.dot(x_t, W_hx) + T.dot(h_t_prev, W_hh) + b_h)\n",
    "    y_t = T.nnet.softmax(T.dot(h_t, W_S) + b_S)\n",
    "    \n",
    "    return [h_t, y_t]\n",
    "\n",
    "[h, y_out], _ = theano.scan(\n",
    "    forward_propagation_step,\n",
    "    sequences=X,\n",
    "    outputs_info=[h0, None],\n",
    "    non_sequences=[W_hx, W_hh, b_h, W_S, b_S],\n",
    "    truncate_gradient=100,\n",
    "    n_steps=X.shape[0]\n",
    ")\n",
    "\n",
    "p_y_given_x = y_out[:, 0, :]\n",
    "\n",
    "y_pred = T.argmax(p_y_given_x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = T.nnet.categorical_crossentropy(p_y_given_x, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dWhx = T.grad(loss, wrt=W_hx)\n",
    "dWhh = T.grad(loss, wrt=W_hh)\n",
    "dbh = T.grad(loss, wrt=b_h)\n",
    "dWS = T.grad(loss, wrt=W_S)\n",
    "dbS = T.grad(loss, wrt=b_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward_propagation = theano.function([X], y_out)\n",
    "loss_calculation = theano.function([X, y], loss)\n",
    "predict = theano.function([X], y_pred)\n",
    "# bbtt = theano.function([X, y], [dWhx, dWhh, dbh, dWS, dbS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = T.scalar('alpha')\n",
    "\n",
    "updates = [\n",
    "    (W_hx, W_hx - alpha * dWhx),\n",
    "    (W_hh, W_hh - alpha * dWhh),\n",
    "    (b_h, b_h - alpha * dbh),\n",
    "    (W_S, W_S - alpha * dWS),\n",
    "    (b_S, b_S - alpha * dbS)\n",
    "]\n",
    "\n",
    "gradient_step = theano.function(\n",
    "    inputs=[X, y, alpha],\n",
    "    outputs=loss,\n",
    "    updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for char in corpus:\n",
    "    X_train.append(char)\n",
    "    y_train.append(np.where(char == 1)[0][0])\n",
    "\n",
    "X_train = np.vstack(X_train[:-1])\n",
    "y_train = np.array(y_train[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(1000, start=1):  # We train for epochs times\n",
    "    for j in xrange(y_train.shape[0], 10):\n",
    "        gradient_step(X_train[j:j+10], y_train[j:j+10], 0.001)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print >> sys.stderr, \"Loss for iteration {}: {}\".format(\n",
    "            i, loss_calculation(X_train, y_train)\n",
    "        )\n",
    "    \n",
    "        # Generate a 1000 characters text\n",
    "        \n",
    "        random_char = corpus.characters[np.random.randint(28, 82)]\n",
    "        \n",
    "        characters = [(\n",
    "                random_char,\n",
    "                corpus.character_encoder(random_char)\n",
    "            )]\n",
    "        # The first character is alphabetic random\n",
    "        \n",
    "        for j in xrange(1000):\n",
    "            char_vectors = np.vstack([vector for char, vector in characters])\n",
    "            next_char_index = predict(char_vectors)[-1]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
