{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron with Python and Theano for Document Classification\n",
    "\n",
    "In this Python notebook, for self-teaching purposes, I will develop a MultiLayer Perceptron and use it later to train a Bag-of-Words text classifier for the 20 newsgroup dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from nltk import corpus\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction import text\n",
    "from theano import sparse\n",
    "\n",
    "np.random.seed(123)  # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the 20 Newsgroups dataset and processing it\n",
    "\n",
    "We continue by fetching and processing the 20 Newsgroup dataset, all the subsets, filter the stopwords and convert each document in a Bag-of-Words with CountVectorizer. For simplicity, we consideration words formed only by lower case letters (with no numbers nor punctuation symbols), and we only take the 10000 most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_newsgroups_document(document):\n",
    "    # To simplify, we ignore everything thas isn't a word\n",
    "    document = re.sub(r\"[^a-zA-Z]\", \" \", document)\n",
    "\n",
    "    # We only make use of lower case words\n",
    "    words = document.lower().split()\n",
    "    \n",
    "    # We filter out every stopword for the english language\n",
    "    stopwords = set(corpus.stopwords.words(\"english\"))\n",
    "    document = \" \".join([word for word in words if word not in stopwords])\n",
    "\n",
    "    return document\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "vectorizer = text.CountVectorizer(analyzer='word', preprocessor=process_newsgroups_document, max_features=10000)\n",
    "newsgroups_dataset = vectorizer.fit_transform(newsgroups.data[:1000]).todense().astype(theano.config.floatX)\n",
    "newsgroups_target = newsgroups.target[:1000]\n",
    "ng_X_train, ng_X_test, ng_y_train, ng_y_test = train_test_split(newsgroups_dataset, newsgroups_target, test_size=0.2)\n",
    "\n",
    "# Convert the data to theano shared variables\n",
    "ng_X_train = theano.shared(ng_X_train, borrow=True)\n",
    "ng_y_train = theano.shared(ng_y_train, borrow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Parameters\n",
    "\n",
    "We define all the parameters for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = newsgroups_dataset.shape[0]  # Number of examples in the dataset.\n",
    "n_input = newsgroups_dataset.shape[1]  # Number of features of the dataset. Input of the Neural Network.\n",
    "n_output = len(newsgroups.target_names)  # Number of classes in the dataset. Output of the Neural Network.\n",
    "n_h1 = 5000  # Size of the first layer\n",
    "n_h2 = 2500  # Size of the second layer\n",
    "alpha = 0.01  # Learning rate parameter\n",
    "lambda_reg = 0.01  # Lambda value for regularization\n",
    "epochs = 1000  # Number of epochs for gradient descent\n",
    "batch_size = 64  # Size of the minibatches to perform sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theano's Computation Graph\n",
    "\n",
    "We have to define the computation graph from Theano.\n",
    "\n",
    "First we define the symbolic variables for the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stateless variables to handle the input\n",
    "index = T.lscalar('index')  # Index of a minibatch\n",
    "X = T.matrix('X')\n",
    "y = T.lvector('y')\n",
    "\n",
    "# First layer weight matrix and bias\n",
    "W1 = theano.shared(\n",
    "    value=np.random.uniform(size=(n_input, n_h1)).astype(theano.config.floatX),\n",
    "    name='W1',\n",
    "    borrow=True\n",
    ")\n",
    "b1 = theano.shared(\n",
    "    value=np.zeros((n_h1,), dtype=theano.config.floatX),\n",
    "    name='b1',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# Second layer weight matrix and bias\n",
    "W2 = theano.shared(\n",
    "    value=np.random.uniform(size=(n_h1, n_h2)).astype(theano.config.floatX),\n",
    "    name='W2',\n",
    "    borrow=True\n",
    ")\n",
    "b2 = theano.shared(\n",
    "    value=np.zeros((n_h2,), dtype=theano.config.floatX),\n",
    "    name='b2',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "# Output layer weight matrix and bias\n",
    "W3 = theano.shared(\n",
    "    value=np.random.uniform(size=(n_h2, n_output)).astype(theano.config.floatX),\n",
    "    name='W3',\n",
    "    borrow=True\n",
    ")\n",
    "b3 = theano.shared(\n",
    "    value=np.zeros((n_output,), dtype=theano.config.floatX),\n",
    "    name='b3',\n",
    "    borrow=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the flow for forward propagation. This means, we define all the activation layers.\n",
    "\n",
    "The activation layers `a1` and `a2` are non-linearities (we use tanh, but could also be sigmoid, relu or another nonlinearity) while the activation `a3` is really the output of our network so it is a Softmax layer.\n",
    "\n",
    "Finally `y_pred` returns the actual class, by getting the argmax value of the softmax for all the N examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z1 = T.dot(X, W1) + b1  # Size: N x n_h1\n",
    "a1 = T.tanh(z1)  # Size: N x n_h1\n",
    "\n",
    "z2 = T.dot(a1, W2) + b2  # Size: N x n_h2\n",
    "a2 = T.tanh(z2)  # Size: N x n_h2\n",
    "\n",
    "z3 = T.dot(a2, W3) + b3  # Size: N x n_output\n",
    "y_out = T.nnet.softmax(z3)  # Size: N x n_output\n",
    "\n",
    "y_pred = T.argmax(y_out, axis=1) # Size: N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our computation graph ready, we only need to define our loss function, for which we use the negative log likelihood, also known as the cross entropy. We also add a regularization term to avoid overfitting of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regularization term to sum in the loss function\n",
    "loss_reg = 1./N * lambda_reg/2 * (T.sum(T.sqr(W1)) + T.sum(T.sqr(W2)) + T.sum(T.sqr(W3)))\n",
    "\n",
    "# Loss function\n",
    "loss = T.nnet.categorical_crossentropy(y_out, y).mean() + loss_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined all the steps for our forward propagation, however we still need to define the function in Theano that does the forward propagation. As the variables, all the functions in Theano are symbolic, we need to define them by setting up their inputs, their outputs and, if we want, we can also define extra information like updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "forward_propagation = theano.function([X], y_out)\n",
    "loss_calculation = theano.function([X, y], loss)\n",
    "predict = theano.function([X], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functions are now callable as they have been compiled by Theano. As the weight matrices have been initialized randomly (i.e. we still have to train them), if we try to predict the values of a couple of instances, most likely we will end up with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The probabilities for each class given the first 2 examples of the newsgroup dataset\n",
    "print forward_propagation(newsgroups_dataset[:2])\n",
    "\n",
    "# The prediction for each of this 2 examples\n",
    "print predict(newsgroups_dataset[:2])\n",
    "\n",
    "# The loss function value for each of this 2 examples (most likely a high one)\n",
    "print loss_calculation(newsgroups_dataset[:2], newsgroups.target[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the forward propagation and the loss function all set, we need to train the network in order to better classify the documents. In order to do this, we first need to get the gradients of the matrices and bias vectors. Theano do this for us (instead of having to make use of backpropagation to calculate the gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dJdW1 = T.grad(loss, wrt=W1)\n",
    "dJdb1 = T.grad(loss, wrt=b1)\n",
    "dJdW2 = T.grad(loss, wrt=W2)\n",
    "dJdb2 = T.grad(loss, wrt=b2)\n",
    "dJdW3 = T.grad(loss, wrt=W3)\n",
    "dJdb3 = T.grad(loss, wrt=b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all the weight matrices and bias vectors are defined as Theano's shared variables, they can be updated in functions. We define then a `gradient_step` function to do so.\n",
    "\n",
    "The `gradient_step` actually uses minibatch stochastic gradient descent to provide a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "updates = [\n",
    "    (W1, W1 - alpha * dJdW1),  # Update step. W1 = W1 - alpha * dJdW1\n",
    "    (b1, b1 - alpha * dJdb1),  # Update step. b1 = b1 - alpha * dJdb1\n",
    "    (W2, W2 - alpha * dJdW2),  # Update step. W2 = W2 - alpha * dJdW2\n",
    "    (b2, b2 - alpha * dJdb2),  # Update step. b2 = b2 - alpha * dJdb2\n",
    "    (W3, W3 - alpha * dJdW3),  # Update step. W3 = W3 - alpha * dJdW3\n",
    "    (b3, b3 - alpha * dJdb3),  # Update step. b3 = b3 - alpha * dJdb3\n",
    "]\n",
    "\n",
    "gradient_step = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=loss,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        X: ng_X_train[index * batch_size: (index + 1) * batch_size],\n",
    "        y: ng_y_train[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the optimization algorithm, using gradient descent, in this first approach we train with the full batch. This is not a good idea however, since will lead to performance issues. We will then retry this with a better approach using minibatches stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches = ng_X_train.get_value().shape[0] / batch_size\n",
    "\n",
    "for i in xrange(epochs):  # We train for epochs times\n",
    "    for mini_batch in xrange(train_batches):\n",
    "        gradient_step(train_batches)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print \"Loss for iteration {}: {}\".format(\n",
    "            i, loss_calculation(ng_X_train.get_value(), ng_y_train.get_value())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
